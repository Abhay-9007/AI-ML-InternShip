# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xBUj46m32CAYHoH53kezfs75Ck1tf-Gl
"""

import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

image_path = "s1.jpg"

# if not os.path.exists(image_path):
#     print(f"❌ File not found at: {os.path.abspath(image_path)}")
# else:
#     # Try reading the image
#     image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

#     # Verify loading
#     if image is None:
#         print("❌ OpenCV could not read the image. Try using PIL instead.")
#     else:
#         print("✅ Image loaded successfully!")
#         print("Matrix shape:", image.shape)
#         print("Data type:", image.dtype)

#         # Display
#         plt.imshow(image, cmap='gray')
#         plt.title("X-ray Image")
#         plt.axis('off')
#         plt.show()

image_path = "s1.jpg"

# Read the image in grayscale (1 channel)
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

print(image)
# Show the image
plt.imshow(image, cmap='gray')
plt.title("X-ray Image")
plt.axis('off')
plt.show()

# Convert image to numpy matrix
matrix = np.array(image)

print("Matrix shape:", matrix.shape)
for i in matrix:
    print(i)

# import os
# import shutil
# from sklearn.model_selection import train_test_split

# # Paths
# base_dir = "data"  # your main folder
# output_dir = "dataset"  # where you want train/val/test folders

# # Create output structure
# for split in ["train", "val", "test"]:
#     for category in ["fracture", "normal"]:
#         os.makedirs(os.path.join(output_dir, split, category), exist_ok=True)

# # Split ratios
# train_ratio = 0.7
# val_ratio = 0.15
# test_ratio = 0.15

# for category in ["fracture", "normal"]:
#     # Get all file names
#     files = os.listdir(os.path.join(base_dir, category))
#     files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]

#     # Shuffle and split
#     train_files, temp_files = train_test_split(files, test_size=(1-train_ratio), random_state=42)
#     val_files, test_files = train_test_split(temp_files, test_size=(test_ratio / (test_ratio + val_ratio)), random_state=42)

#     # Copy files
#     for f in train_files:
#         shutil.copy(os.path.join(base_dir, category, f), os.path.join(output_dir, "train", category))
#     for f in val_files:
#         shutil.copy(os.path.join(base_dir, category, f), os.path.join(output_dir, "val", category))
#     for f in test_files:
#         shutil.copy(os.path.join(base_dir, category, f), os.path.join(output_dir, "test", category))

# print("✅ Dataset successfully split into train/val/test folders!")

!ls

import os
import pathlib
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import classification_report, confusion_matrix

# reproducibility (optional)
SEED = 42
tf.random.set_seed(SEED)
np.random.seed(SEED)

# Basic GPU check (optional)
print("TF version:", tf.__version__)
print("GPU available:", tf.config.list_physical_devices('GPU'))

from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

def load_and_show(path, size=(224,224), cmap='gray'):
    img = Image.open(path).convert('L')  # grayscale
    img = img.resize(size)
    arr = np.array(img) / 255.0   # normalize to [0,1]
    plt.imshow(arr, cmap=cmap)
    plt.title(os.path.basename(path))
    plt.axis('off')
    plt.show()
    return arr

# Example: replace with a real path in your notebook
# preview = load_and_show("dataset/train/normal/sample1.jpg")

DATA_DIR = pathlib.Path("dataset")   # change if needed
IMG_SIZE = (224, 224)   # change to desired size (224 is common)
BATCH_SIZE = 32
AUTOTUNE = tf.data.AUTOTUNE

train_dir = DATA_DIR / "train"
val_dir   = DATA_DIR / "val"
test_dir  = DATA_DIR / "test"

train_ds_raw = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    labels='inferred',
    label_mode='binary',            # binary classification
    color_mode='grayscale',         # X-rays are grayscale
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    seed=SEED,
    shuffle=True
)

val_ds_raw = tf.keras.utils.image_dataset_from_directory(
    val_dir,
    labels='inferred',
    label_mode='binary',
    color_mode='grayscale',
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    seed=SEED,
    shuffle=False
)

test_ds_raw = tf.keras.utils.image_dataset_from_directory(
    test_dir,
    labels='inferred',
    label_mode='binary',
    color_mode='grayscale',
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    seed=SEED,
    shuffle=False
)

# Store class names before preparing the dataset
class_names = train_ds_raw.class_names

# Show class names
print("Class names:", class_names)

# Normalization layer expects float inputs in [0,1], but image_dataset returns uint8 => we cast
normalization_layer = layers.Rescaling(1./255)

data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),              # horizontal flip
    layers.RandomRotation(0.05),                  # small rotation
    layers.RandomZoom(0.05),
], name="data_augmentation")

def prepare(ds, augment=False):
    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTOTUNE)
    if augment:
        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)
    ds = ds.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=AUTOTUNE)
    return ds.cache().prefetch(buffer_size=AUTOTUNE)

train_ds = prepare(train_ds_raw, augment=True)
val_ds   = prepare(val_ds_raw, augment=False)
test_ds  = prepare(test_ds_raw, augment=False)

def make_model(input_shape=(*IMG_SIZE, 1)):
    inputs = keras.Input(shape=input_shape)
    x = layers.Conv2D(32, 3, activation="relu", padding="same")(inputs)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Conv2D(64, 3, activation="relu", padding="same")(x)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Conv2D(128, 3, activation="relu", padding="same")(x)
    x = layers.MaxPooling2D(2)(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(64, activation="relu")(x)
    outputs = layers.Dense(1, activation="sigmoid")(x)  # binary
    model = keras.Model(inputs, outputs, name="scaphoid_cnn")
    return model

model = make_model()
model.summary()

# Optional: compute class weights if classes are imbalanced
def compute_class_weights(dataset):
    # dataset yields batches (images, labels)
    labels = np.concatenate([y.numpy().ravel() for x, y in dataset], axis=0)
    # count
    from sklearn.utils import class_weight
    weights = class_weight.compute_class_weight("balanced", classes=np.unique(labels), y=labels)
    return {int(c): w for c, w in zip(np.unique(labels), weights)}

# If you want to compute weights use this:
# class_weights = compute_class_weights(train_ds.unbatch().batch(1024))
# print(class_weights)

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    loss="binary_crossentropy",
    metrics=[keras.metrics.BinaryAccuracy(name="accuracy"),
             keras.metrics.AUC(name="auc")]
)

EPOCHS = 12
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    # class_weight=class_weights,  # uncomment if computed
)

# Get predictions and labels for the entire test set
y_true = []
y_pred = []
# class_names = sorted(os.listdir("dataset/test")) # This line is no longer needed

for images, labels in test_ds:
    preds = model.predict(images, verbose=0)
    y_true.extend(labels.numpy().ravel().tolist())
    y_pred.extend((preds.ravel() > 0.5).astype(int).tolist())

print("Confusion Matrix:")
print(confusion_matrix(y_true, y_pred))
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

plt.figure(figsize=(10,4))

# Accuracy
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.title('Accuracy')
plt.legend()

# Loss
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.title('Loss')
plt.legend()

plt.show()

# Function to Predict on a single image...

from tensorflow.keras.utils import load_img, img_to_array

img_path = "s2.jpg"  # put your image path here
img = load_img(img_path, target_size=(224,224), color_mode='grayscale') # Added color_mode='grayscale'
img_array = img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

prediction = model.predict(img_array)[0][0]

if prediction > 0.5:
    print("🩻 Predicted: Scaphoid Injury")
else:
    print("✅ Predicted: Normal")

MODEL_PATH = "scaphoid_detector.keras"
model.save(MODEL_PATH, include_optimizer=False)
print("Saved model to", MODEL_PATH)

